---
title: "Conformal Inference with Tidymodels"
author: "Max Kuhn"
---

```{r}
#| label: setup
#| include: false
library(tidymodels)
library(gganimate)
library(probably)
library(rules)
library(splines)

# ------------------------------------------------------------------------------

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)

# ------------------------------------------------------------------------------

# from https://www.rdatagen.net/post/generating-non-linear-data-using-b-splines/
gen_spline <- function(n, degree = 3, std_dev = .05) {
  knots <- c(0.333, 0.666)
  theta = c(0.2, 0.8, -0.1, 1.2, 0.3, 0.8)
  x <- sort(runif(n))
  basis <- splines::bs(x = x, knots = knots, degree = degree,
                       Boundary.knots = c(0,1), intercept = TRUE)
  y.spline <- basis %*% theta
  dt <- tibble(predictor = x, f = as.vector(y.spline))
  dt$outcome <- dt$f + rnorm(nrow(dt), sd = std_dev)
  dt$f <- NULL
  dt
}
set.seed(5)
train_data  <- gen_spline(1000)
cal_data    <- gen_spline(500)
test_data   <- gen_spline(500)

grid_data <- tibble(predictor = seq(0, 1, length.out = 1000))

# ------------------------------------------------------------------------------

spline_rec <- 
  recipe(outcome ~ predictor, data = train_data) %>% 
  step_spline_natural(predictor, deg_free = 10)

spline_wflow <- workflow(spline_rec, linear_reg())
spline_fit <- spline_wflow %>% fit(data = train_data)
pred_val <- augment(spline_fit, cal_data) %>% mutate(.resid = outcome - .pred)
val_n <- nrow(pred_val)
quant_90 <- quantile(pred_val$.resid, probs = c(0.05, 0.95))
y_rng <- c(0.085, 0.90)
outlier_at <- -0.14
```


# or... how to make prediction intervals with no parameteric asssumptions


## Um OK. What's a prediction interval? 

Prediction interval with level $\alpha$: 

> A range of values that is likely to contain the value of a single new _observation_ with probability $1-\alpha$. 

It gives a sense of the variability in a new prediction. 

## Let's start with some data...


```{r}
#| label: final-data
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-align: "center" 

pred_val %>% 
  ggplot(aes(x = .resid)) + 
  geom_histogram(col = "white", bins = 25) + 
  geom_rug(alpha = 1/ 4) +
  labs(title = sample~size~n[c]~'=500') +
  labs(x = "Sample Values", y = NULL) +
  coord_flip()
```

## Is this new data point discordant?

```{r}
#| label: discordant-data
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-align: "center" 

pred_val %>% 
  ggplot(aes(x = .resid)) + 
  geom_histogram(col = "white", bins = 25) + 
  geom_rug(alpha = 1/ 4) +
  labs(title = sample~size~n[c]+1) +
  labs(x = "Sample Values", y = NULL) +
  geom_segment(
    data = tibble(x = outlier_at, xend = outlier_at, y = 30, yend = 0),
    aes(x = x, xend = xend, y = y, yend = yend),
    col = "#C34A36",
    linewidth = 1.2,
    arrow = arrow(length = unit(0.03, "npc"))
  ) +
  coord_flip()
```



## A Simple Probability Statement

Without making parametric assumptions, we could say

$$Pr[Q_L< Y_{n_c} < Q_U] = 1 - \alpha$$
where $Q_L$ and $Q_U$ are quantiles excluding $\alpha/2$ tail areas. 

<br> 

So, for some $\alpha$, we could say that new data between $Q_L$ and $Q_U$ are "likely" to conform to our original, reference distribution. 


## Use quantiles to define "conformity"

```{r}
#| label: data-quantiles
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-align: "center" 

pred_val %>% 
  ggplot(aes(x = .resid)) + 
  geom_histogram(col = "white", bins = 25) + 
  geom_rug(alpha = 1/ 4) +
  labs(title = sample~size~n[c]+1) +
  labs(x = "Sample Values", y = NULL) +
  geom_vline(xintercept = quant_90[1], col = "#00C9A7", linewidth = 1.2) +
  geom_vline(xintercept = quant_90[2], col = "#00C9A7", linewidth = 1.2) +
  annotate("text", y = 50, x = quant_90[1] * 1.4, label = "Lower 5% Quantile", 
           col = "#00C9A7") +
  annotate("text", y = 50, x = quant_90[2] * 1.4, label = "Upper 5% Quantile", 
           col = "#00C9A7") +  
  geom_segment(
    data = tibble(x = outlier_at, xend = outlier_at, y = 30, yend = 0),
    aes(x = x, xend = xend, y = y, yend = yend),
    col = "#C34A36",
    linewidth = 1.2,
    arrow = arrow(length = unit(0.03, "npc"))
  ) +
  coord_flip()
```


## What if the samples were residuals?

```{r}
#| label: residual-quantiles
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-align: "center" 

pred_val %>% 
  ggplot(aes(x = .resid)) + 
  geom_histogram(col = "white", bins = 25) + 
  geom_rug(alpha = 1/ 4) +
  labs(title = sample~size~n[c]~'=500') +
  labs(x = "Out-of-Sample Residuals", y = NULL) +
  geom_vline(xintercept = quant_90[1], col = "#00C9A7", linewidth = 1.2) +
  geom_vline(xintercept = quant_90[2], col = "#00C9A7", linewidth = 1.2)  +
  coord_flip()
```


## Linear regression with spline features


```{r}
#| label: fit-only
#| echo: false
#| out-width: 90%
#| fig-width: 6
#| fig-height: 4.25
#| fig-align: "center" 

augment(spline_fit, train_data) %>% 
  ggplot(aes(predictor)) + 
  geom_point(aes(y = outcome), alpha = .2, cex = 1) + 
  geom_line(aes(y = .pred), col = "#D16BA5", linewidth = 1) + 
  labs(title = Training~Set:~n[tr]~'=1000') +
  lims(y = y_rng)
```

## Compute residuals on other data

```{r}
#| label: resid
#| echo: false
#| out-width: 90%
#| fig-width: 6
#| fig-height: 4.25
#| fig-align: "center" 

augment(spline_fit, cal_data) %>% 
  ggplot(aes(x = predictor)) + 
  geom_segment(aes(xend = predictor, yend = .pred, y = outcome), alpha = .5) +
  geom_point(aes(y = outcome), alpha = .3, cex = .1) + 
  geom_line(aes(y = .pred), col = "#D16BA5", linewidth = 1) + 
  labs(title = Calibration~Set:~n[c]~'=500') +
  lims(y = y_rng)
```


## Compute interval of "conforming" values

```{r}
#| label: residual-quantiles2
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
#| fig-align: "center" 

pred_val %>% 
  ggplot(aes(x = .resid)) + 
  geom_histogram(col = "white", bins = 25) + 
  geom_rug(alpha = 1/ 4) +
  labs(title = Calibration~Set:~n[c]~'=500') +
  labs(x = "Calibration Residuals", y = NULL) +
  geom_vline(xintercept = quant_90[1], col = "#00C9A7", linewidth = 1.2) +
  geom_vline(xintercept = quant_90[2], col = "#00C9A7", linewidth = 1.2)  +
  coord_flip()
```


## Center interval around predictions

```{r}
#| label: fit
#| echo: false
#| out-width: 90%
#| fig-width: 6
#| fig-height: 4.25
#| fig-align: "center" 

augment(spline_fit, grid_data) %>% 
  mutate(
    .pred_lower = .pred - quant_90[1],
    .pred_upper = .pred - quant_90[2]
  ) %>% 
  ggplot(aes(predictor)) + 
  geom_point(data = test_data, aes(y = outcome), alpha = .2, cex = 1) + 
  geom_line(aes(y = .pred), col = "#D16BA5", linewidth = 1) + 
  geom_line(aes(y = .pred_lower), col = "#00C9A7", linewidth = 1.2) + 
  geom_line(aes(y = .pred_upper), col = "#00C9A7", linewidth = 1.2) +  
  labs(title = Test~Set:~n[c]~'=500') +
  lims(y = y_rng)
```



## Is this a prediction interval? 

Conformal intervals are using a completely different approach to produce intervals with _average_ coverage of  $1-\alpha$ 

(usually - for many conformal methods)

<br> 

_For the statisticians out there_: the methods have a strong Frequentist theme similar to nonparameteric inference. 



## Pros

* Basic methods assume exchangeability of the data. 
  - Specialized methods for time series (see the [modeltime package](https://business-science.github.io/modeltime/reference/modeltime_forecast.html)!).
* Can work with any regression or classification model.
  - We've only implemented it for regression models so far.
* Relatively fast (except for "full" conformal inference).

## Cons

* Extrapolating beyond training/calibration sets is problematic
  - Some methods may not reflect the extrapolation in the interval width. 
  - Other conformal methods can produce [especially bad results](https://www.tidymodels.org/learn/models/conformal-regression/#the-major-downside)
  - The applicable package can be a big help identifying extrapolation.
* Probably not great for small sample sizes.

## Setup the data

```{r}
#| label: data-splits
library(tidymodels)

# These are simulated data:
nrow(train_data)
nrow(cal_data)
nrow(test_data)

# Setup some resampling for later use
set.seed(322)
cv_folds <- vfold_cv(train_data, v = 10, strata = outcome)
```


## A support vector machine model

```{r}
#| label: svm
svm_spec <- 
  svm_rbf() %>% 
  set_mode("regression")

svm_wflow <- workflow(outcome ~ predictor, svm_spec)

svm_fit <- svm_wflow %>% fit(data = train_data)
```

<br> 

Now let's look at three functions for producing intervals...

## Split Conformal Inference

```{r}
#| label: svm-fit
#| code-line-numbers: "|1,3|6|9-13|"
library(probably)

conf_split <- int_conformal_split(svm_fit, cal_data = cal_data)

conf_split_test <- 
  predict(conf_split, test_data, level = 0.90) %>% 
  bind_cols(test_data)

conf_split_test %>% slice(1)
```

## Split Conformal Inference

```{r}
#| label: svm-split
#| echo: false
#| out-width: 90%
#| fig-width: 6
#| fig-height: 4.25
#| fig-align: "center" 

conf_split_pred <- 
  predict(conf_split, grid_data, level = 0.90) %>% 
  bind_cols(grid_data)

conf_split_pred %>% 
  ggplot(aes(predictor)) + 
  geom_point(data = test_data, aes(y = outcome), alpha = .2, cex = 1) + 
  geom_line(aes(y = .pred), col = "#D16BA5", linewidth = 1) + 
  geom_line(aes(y = .pred_lower), col = "#00C9A7", linewidth = 1.2) + 
  geom_line(aes(y = .pred_upper), col = "#00C9A7", linewidth = 1.2)  +  
  labs(title = "Test Set") +
  lims(y = y_rng)
```

## CV+ Inference

```{r}
#| label: cv-plus
#| code-line-numbers: "|1-2|4-6|8|11|"
# 'extract' to save the 10 fitted models
ctrl <- control_resamples(save_pred = TRUE, extract = I)

svm_resampled <- 
  svm_wflow %>% 
  fit_resamples(resamples = cv_folds, control = ctrl)

conf_cv <- int_conformal_cv(svm_resampled)

conf_cv_test <- 
  predict(conf_cv, test_data, level = 0.90) %>% 
  bind_cols(test_data)
```

## CV+ Inference

```{r}
#| label: svm-cv
#| echo: false
#| out-width: 90%
#| fig-width: 6
#| fig-height: 4.25
#| fig-align: "center" 

conf_cv_pred <- 
  predict(conf_cv, grid_data, level = 0.90) %>% 
  bind_cols(grid_data) %>% 
  mutate(width = .pred_upper - .pred_lower, method = "CV+")

conf_cv_pred %>% 
  ggplot(aes(predictor)) + 
  geom_point(data = test_data, aes(y = outcome), alpha = .2, cex = 1) + 
  geom_line(aes(y = .pred), col = "#D16BA5", linewidth = 1) + 
  geom_line(aes(y = .pred_lower), col = "#00C9A7", linewidth = 1.2) + 
  geom_line(aes(y = .pred_upper), col = "#00C9A7", linewidth = 1.2)  +  
  labs(title = "Test Set") +
  lims(y = y_rng)
```

## Conformalized quantile regression

```{r}
#| label: cv-quant
#| code-line-numbers: "|5-9|12|"
# We have to pass the data sets and pre-set 
# the interval coverage:
set.seed(837)
conf_qntl <-
  int_conformal_quantile(svm_fit,
                         train_data = train_data,
                         cal_data = cal_data,  #<- split sample 
                         level = 0.90,
                         ntree = 2000)

conf_qntl_test <- 
  predict(conf_qntl, test_data) %>% 
  bind_cols(test_data)
```

## Conformalized quantile regression

```{r}
#| label: svm-quantile
#| echo: false
#| out-width: 90%
#| fig-width: 6
#| fig-height: 4.25
#| fig-align: "center" 

conf_qntl_pred <- 
  predict(conf_qntl, grid_data) %>% 
  bind_cols(grid_data) %>% 
  mutate(width = .pred_upper - .pred_lower)

conf_qntl_pred %>% 
  ggplot(aes(predictor)) + 
  geom_point(data = test_data, aes(y = outcome), alpha = .2, cex = 1) + 
  geom_line(aes(y = .pred), col = "#D16BA5", linewidth = 1) + 
  geom_line(aes(y = .pred_lower), col = "#00C9A7") + 
  geom_line(aes(y = .pred_upper), col = "#00C9A7")  +  
  labs(title = "Test Set") +
  lims(y = y_rng)
```

## Does it work? 

```{r}
#| label: coverage
#| include: false

coverage <- function(x) {
    res <- mean(x$outcome >= x$.pred_lower & x$outcome <= x$.pred_upper)
    round(res * 100, 1)
}
```

For our test set, the coverage for 90% predictions intervals: 

 * Split conformal: `r coverage(conf_split_test)`%
 * CV+: `r coverage(conf_cv_test)`%
 * Conformalized quantile regression: `r coverage(conf_qntl_test)`%

I also did a lot of simulations to make sure that the coverage was on-target. 

These can be found at [`https://github.com/topepo/conformal_sim`](https://github.com/topepo/conformal_sim). 

## 

What's next? 

 - Classification models
   * The focus is often to cluster predicted classes that have equivocal probabilities. 
 - New methodologies as they pop up. 


Thanks: the tidymodels group, Joe Rickert, and the conference committee. 

## Learning More

* [An article](https://www.tidymodels.org/learn/models/conformal-regression/) on `tidymodels.org`
* [_Introduction To Conformal Prediction With Python_](https://christophmolnar.com/books/conformal-prediction/) by Christoph Molnar
 (highly recommended)
* [`awesome-conformal-prediction`](https://github.com/valeman/awesome-conformal-prediction) on GitHub. 
* [Ryan Tibshirani's notes](https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/conformal.pdf) (pdf)
* Angelopoulos, Anastasios N., and Stephen Bates. "[A gentle introduction to conformal prediction and distribution-free uncertainty quantification](https://arxiv.org/abs/2107.07511)." arXiv preprint arXiv:2107.07511 (2021).


